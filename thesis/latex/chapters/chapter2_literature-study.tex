% !TeX root = thesis.tex

\chapter{Literature study}\label{ch:literature-study}
In this chapter, we discuss existing research relevant to uncertainty in \gls{ai} models and how it can be mapped to real-world scenarios.
First, we will elaborate on what uncertainty is and different ways it can sneak in \gls{ai} models.
Next, we will discuss how this uncertainty can be quantified.
After that, we will dive into different existing methods to make \gls{ai} models uncertainty-aware and discuss how their ideas can be applied to object detection.
Following that, we will look into ways data is gathered to train and evaluate uncertainty-aware models.
To close, we will briefly discuss safety mechanisms that can be implemented to prevent a robot from taking actions based on uncertain predictions.

\section{Uncertainty in AI} \label{ch:literature-study_section:uncertainty-in-ai}
There are lots of complex systems that need to be modeled and understood.
These systems can be modelled using \gls{ai} models, however these are not perfect and can make mistakes.
While moddeling such a system and gathering data about it, there are multiple sources of uncertainty that can sneak in~\cite{src:survey-uncertainty-dnn}.
\begin{itemize}
    \item The variability in real world situations
    \item The errors inherent to the measurement systems
    \item The errors in the architecture specification of the DNN
    \item The errors in the training procedure of the DNN
    \item The errors caused by unknown data
\end{itemize}
All these unsertainties can result in a model making wrong predictions.
When in an academic or non safety-critical settings, these mistakes do not exist, can be mittigated or accapted.
For example, the collected data will be very clean as the data gathering process is done in a controlled environment with no real variability.
The systems are mostly developed and used in a controlled environment, for example a lab setting, where the lighting is always the same, there is no unexpected occlusions, etc.

However, when deploying \gls{ai} models in real-world safety-critical settings, these mistakes can have severe consequences.

\subsection{Variability in real world situations} \label{ch:literature-study_section:uncertainty-in-ai_subsection:variability-in-real-world-situations}
The real world changes constantly, resulting in different settings where objects can behave / look differently. 
All situations should be covered sufficiently by the training set. 
If not, the network trained on the data can not garentee a good output for every new real world situation. 
A distribition shift occurs when real world situations differ from the training set. 
These shifts are very hard for a NN to grasp, causing its performence to change significantly.

Presume that there is a dataset created for detecting a sertain object in a factory setting at a set machine during the day. 
When a NN is trained on this dataset, it will be very good at detecting this object in that specific setting. 
However, it might happen that, because of a sudden increase in orders, the machine needs to run overtime and work during the night, moreover more machines at different locations need to help out. 
Because the dataset does not cover the objects at night, nor the settings of the other machines (lighting is different, shadows are cast differently, etc.), the trained NN fails to addapt to these new conditions. 
Therefore, its performence drops, which can lead to all kinds of problems.

\subsection{Measurement errors} \label{ch:literature-study_section:uncertainty-in-ai_subsection:measurement-errors}
This comes down to the acquired data having some mistakes in them. 
These can come from limitations of the measurement devices or incorrect / inprecise labeling of the data. 
These mistakes can however be a way to regulize a network, however it needs to be subtle.

To continue the example of the dataset used for object detection on a machine, it could be that the camera used for the capturing of data has a low resolution. 
This could lead to an object, which needed to be detected, getting lost because of the lack of pixels in the image. 
Even more, it could lead to incorrect labeling of an object, as it is labeled as object a, when it is in fact object b. 
Finally it can lead to the bounding box not precisely being placed around the object.

\subsection{Architecture specification errors} \label{ch:literature-study_section:uncertainty-in-ai_subsection:architecture-specification-errors}
The structure of a network differs from model to model, with each model having its advantages and disadvantages.
Each model will be different and thus its output will have a sertain level of unsertainty.

Suppose the factory requires an object detection model to best fit their need. 
There are a lot of models available, think of any CNN network, U-net, etc. 
Each model will have its own capabilities and thus unsertainty.

\subsection{Training procedure errors} \label{ch:literature-study_section:uncertainty-in-ai_subsection:training-procedure-errors}
During the training of a model, there are much decisions that need to be made. 
The learning rate, the number of epochs, the way of using the data, the optimizer used, the weight initialization, etc. 
Each combination of parameters will result in a different model with its own errors and unsertainty.

The factory dataset can be split in different batch sizes, ordered in different ways, etc. 
The chosen model can be trained with these different ways of handeling data, using different parameters to optimize the chosen model. 
Again leading to different models who are better in sertain things and worse in other.

\subsection{Errors by unknown data} \label{ch:literature-study_section:uncertainty-in-ai_subsection:errors-by-unknown-data}
When the model is fully trained, it can happen that, during inference, the model encounters a situation it has not seen before. 
This situation was not expected in the original scope of the problem, but the model will need to deal with it, causing unsertainty.

In the factory example, the model is trained for a sertain enviorment and objective: detecting sertain objects in a factory setting with variable lighting conditions and different machines.
If an object that was not expected to be in the enviorment, a bird for example, enters the camera's viewpoint, it can cause the model to be unstertain on what to do, since it has never seen a bird before. 
This is not because the enviorment was insufficiently specified, but because of the bird being there unexpectedly.

\section{Quantifying uncertainty} \label{ch:literature-study_section:quantifying-uncertainty}

\section{Uncertainty-aware AI models} \label{ch:literature-study_section:uncertainty-aware-ai-models}
\subsection{Bayesian Neural Networks} \label{ch:literature-study_section:uncertainty-aware-ai-models_subsection:bayesian-neural-networks}

\subsection{Deep Ensemble Methods} \label{ch:literature-study_section:uncertainty-aware-ai-models_subsection:deep-ensemble-methods}
A deep ensemble method~\cite{src:deep-ensemble-methods} rests on the idea that a combination of (simpler) models can be trained and perform better then a single (complex) model.
Not only that, but it also outputs a great uncertainty score about its predictions.
A typical ensemble method uses multiple decision trees as their models, a deep ensemble method uses, as the name suggests, uses deep \glspl{nn} as its models.
Creating a deep ensemble method contains 3 main steps:
\begin{itemize}
    \item A proper scoring rule, which is used as a way to train the models in the ensemble.
    \item Adversarial training, which is used to make the models in the ensemble more robust.
    \item The actual ensembling, which combines the outputs of the different models to create a final output and uncertainty score.
\end{itemize}
\paragraph{The scoring rule}
A scoring rule is a way to measure the quality of a model's probabilistic predictions.
It should encourage the model to generate outputs which are accurate and are capable of telling when it is uncertain about its output.
To make this a bit more concrete, in a setting where a model needs to predict if there is a disk in frame or not, a great scoring rule should punish the model hard if it predicts a disk to be in frame with a high probability, when in fact the groud truth states there is no disk in frame.
The model should be encouraged to predict a low probability for the disk being in frame, if it is not sure if there is a disk in frame.
It should result in a model which is rightfully confident about its predictions when it is sertain about its predictions, while also being able to express its unsertainty when it is not confident in its predictions.

Fortunately, for classification tasks, the loss functions, which are commonly used to train \glspl{nn}, such as the softmax cross entropy loss, are proper scoring rules.

\paragraph{Adversarial training}
Adversarial training is a technique which is used to make \glspl{nn} more robust against small changes who can drastically change the output of the model.
Usually these changes are so small a human can not observe the difference, but the model is fooled to think it is something completely different.
For example, when classifying images of disks and cubes, when an image is classified as a disk with a high probability, it could be that with a very small change in pixel value, the network all of a sudden thinks it is a cube with high confidence.
This is something we want to avoid at all cost, since this big swing eliminates any notion of uncertainty the model could have had.

Adverserial training tries to fix these big swings by generating new training samples which are very close to some original data (for example a few pixels tweaked) and classified as some other output-class.
Using the example, this means it gets an image of a disk, tweaks a few pixels so the model thinks it is a cube, and adds this new image to the training set with the correct label (disk).
By doing this, the model learns to be more robust against these small changes, and the big swings in output are reduced.

\paragraph{Ensembling}
The final step in creating a deep ensemble method is the actual ensembling.
The paper~\cite{src:deep-ensemble-methods} suggests to create an ensemble of deep \glspl{nn} to predict an output with an uncertainty score.
Each model in the ensemble should be trained independently, however this does not mean they should be different model architectures.
In fact, when randomly initializing the \gls{nn} parameters, along with shuffeling the training data, the networks where different enough to obtain good results.
This makes that no methods like bagging or boosting are needed to create diversity in the ensemble.
Which means all the training data can be used for the training of the models in the ensemble, which is benefitial for the deep \glspl{nn} which need more data to train properly.

To get the final output and uncertainty score, the outputs of the different models are combined.
Each model in the ensemble outputs a probability distribution over the different classes.
These probability distributions are then averaged to get the final output distribution.
The label with the highest probability in this final distribution is taken as the final output with the uncertainty belonging to it.
Lets continue the example of classifying images of disks and cubes to make this more concrete.
Assuming we have an ensemble of 3 models, and they output the following probability distributions for a given input image:
\begin{itemize}
    \item Model 1: [0.7 (disk), 0.3 (cube)]
    \item Model 2: [0.4 (disk), 0.6 (cube)]
    \item Model 3: [0.9 (disk), 0.1 (cube)]
\end{itemize}
Averaging these distributions results in the final output distribution: [0.67 (disk), 0.33 (cube)].
The final output is thus 'disk' with an uncertainty score of 0.67.
\subsection{Monte Carlo dropout} \label{ch:literature-study_section:uncertainty-aware-ai-models_subsection:monte-carlo-dropout}
Dropout is a regularization technique commonly used in the training of \glspl{nn} to prevent overfitting.
The idea is to randomly drop some neurons in the network during the training phase.
This forces the model to use all the available neurons and not rely too much on a small subset of them.
When the model is fully trained and ready for inference, dropout is usually turned off, using all the neurons to make predictions.

\gls{mc} dropout~\cite{src:mc-dropout} expands on this idea by keeping dropout turned on during inference.
Every time a set of neurons is dropped, the network changes a bit, which can result in different outputs for the same input.
Each alteration of the network can be seen as a different model in an ensemble.
By letting the model make multiple forward passes with dropout turned on, we can gather a set of different outputs for the same input with their own predicted probability distributions.
These outputs can then be combined in a similar way as the deep ensemble method in~\cref{ch:literature-study_section:uncertainty-aware-ai-models_subsection:deep-ensemble-methods} to get a final output and uncertainty score.

Reusing the example of classifying images of disks and cubes, assuming we make 3 forward passes with \gls{mc} dropout, we could get the following outputs:
\begin{itemize}
    \item Forward pass 1: [0.8 (disk), 0.2 (cube)]
    \item Forward pass 2: [0.5 (disk), 0.5 (cube)]
    \item Forward pass 3: [0.6 (disk), 0.4 (cube)]
\end{itemize}
Averaging these distributions results in the final output distribution: [0.63 (disk), 0.37 (cube)].
The final output is thus 'disk' with an uncertainty score of 0.63.

\section{Datasets for uncertainty-aware models} \label{ch:literature-study_section:datasets-for-uncertainty-aware-models}

\section{Safety mechanisms for uncertain predictions} \label{ch:literature-study_section:safety-mechanisms-for-uncertain-predictions}