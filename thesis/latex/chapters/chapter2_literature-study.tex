% !TeX root = thesis.tex

\chapter{Literature study}\label{ch:literature-study}
In this chapter, we discuss existing research relevant to uncertainty in \gls{ai} models and how it can be mapped to real-world scenarios.
First, we will elaborate on what uncertainty is and different ways it can sneak in \gls{ai} models.
Next, we will discuss how this uncertainty can be quantified.
After that, we will dive into different existing methods to make \gls{ai} models uncertainty-aware and discuss how their ideas can be applied to object detection.
Following that, we will look into ways data is gathered to train and evaluate uncertainty-aware models.
To close, we will briefly discuss safety mechanisms that can be implemented to prevent a robot from taking actions based on uncertain predictions.

\section{Uncertainty in AI} \label{ch:literature-study_section:uncertainty-in-ai}
There are lots of complex systems that need to be modeled and understood.
These systems can be modelled using \gls{ai} models as they offer a way can grasp the complexity of these systems and operate them in the most efficient / safe way possible.
However, these are, just as humans, not perfect and can make mistakes.
When using normal (non uncertainty-aware) \gls{ai} models, these mistakes are not communicated with the user. 
For example, when a \gls{cnn} is used to detect an object on a conveyor belt, it will output the class of the object it thinks is on the conveyor belt, without any indication of how certain it is about this prediction.
So if the object is only partially visible, the lighting is bad, or the object is very similar to another object, the model might be very uncertain about its prediction, but it will still output a class as if it is certain about it.

All possible uncertainties regarding \glspl{dnn} can be captured in two groups: aleatoric uncertainty and epistemic uncertainty~\cite{src:survey-uncertainty-dnn}~\cite{src:scalable-bayesian-deep-learning}.

\subsection{Aleatoric uncertainty}\label{ch:literature-study_section:uncertainty-in-ai_subsection:aleatoric-uncertainty}
Aleatoric uncertainty is the uncertainty that comes from noise in the data.
Its nature is irreducible, meaning that no matter how much data we gather, this uncertainty will always be present in the data.

\paragraph{Measurement errors}\label{ch:literature-study_section:uncertainty-in-ai_paragraph:measurement-errors}
When gathering data, mistakes can be made when measuring.
These can come from limitations of the measurement devices or incorrect / inprecise labeling of the data. 
An important note to make is that these mistakes can be a way to regulize a network, if it stays subtle.

Let's take a regression problem as an example, where a model needs to predict a pair of \(x\) and \(y\) values for the center of an object.
It could be that the camera used for the capturing of data has a low resolution. 
This could lead to an object, which needed to be detected, getting lost because of the lack of pixels in the image. 
Even more, it could lead to inaccurate annotation of the center of the object, as its center is labeled as \((x_1, y_1)\), when it is in fact \((x_1+\delta, y_1+\epsilon)\) (\(\delta\) and \(\epsilon\) being small errors in the measurement).

\subsection{Epistemic uncertainty}\label{ch:literature-study_section:uncertainty-in-ai_subsection:epistemic-uncertainty}
Epistemic uncertainty is the uncertainty that comes from noise in the model or its training process.
This can have many different causes like the architecture of the model, the training procedure, lack of knowledge because of insufficient or irrelevant training data.
Its nature is, in theory, reducible, meaning that by gathering more / better data, using a better model architecture, etc., this uncertainty can be reduced.

\paragraph{Variability in real world situations} \label{ch:literature-study_section:uncertainty-in-ai_subsection:epistemic-uncertainty_paragraph:variability-in-real-world-situations}
The real world changes constantly, resulting in different settings where objects can behave / look differently. 
All situations should be covered sufficiently by the training set. 
If not, the network trained on the data can not garentee a good output for every new real world situation. 
A distribition shift occurs when real world situations differ from the training set. 
These shifts are very hard for a \gls{dnn} to grasp, causing its performence to change significantly.

Continueing the example of of the regression problem where a model needs to predict the center of an object.
When a \gls{dnn} is trained on a dataset which covers the objects with great lighting, no occlusions, no other objects covering the view, etc., it will be very good at predicting the center of the object in that specific setting. 
However, it might happen that, when using the model, the conditions change (e.g., lighting changes, new objects appear, etc.).
Because the dataset does not cover the new conditions, the trained \gls{dnn} fails to addapt to these new conditions, causing its performence to drop.

\paragraph{Architecture specification errors} \label{ch:literature-study_section:uncertainty-in-ai_subsection:epistemic-uncertainty_paragraph:architecture-specification-errors}
The structure of a network differs from model to model, with each model having its advantages and disadvantages on the performence, and thus the uncertainty of the predictions.

Suppose a factory requires the previously discussed regression model to best fit their need. 
There are a lot of model architectures available, a fully connected network, a \gls{cnn}, a U-net, etc.
Each model has its own number of layers, connections, activation functions, etc.
All these networks will have its their own capabilities and thus uncertainty.

\paragraph{Training procedure errors} \label{ch:literature-study_section:uncertainty-in-ai_subsection:epistemic-uncertainty_paragraph:training-procedure-errors}
During the training of a model, there are much decisions that need to be made. 
The learning rate, the number of epochs, the way of using the data, the optimizer used, the weight initialization, etc. 
Each combination of parameters will result in a different model with its own errors and uncertainty.

The dataset for regression of an object center can be split in different batch sizes, ordered in different ways, etc. 
The chosen model can be trained with these different ways of handeling data, using different parameters to optimize the chosen model. 
Again leading to different models who are better in sertain things and worse in other.

\paragraph{Errors by unknown data} \label{ch:literature-study_section:uncertainty-in-ai_subsection:epistemic-uncertainty_paragraph:errors-by-unknown-data}
When the model is fully trained, it can happen that, during inference, the model encounters a situation it has not seen before. 
This situation was not expected in the original scope of the problem, but the model will need to deal with it, causing uncertainty.

In the factory example, the model is trained for a sertain enviorment and objective: detecting the center of objects in a factory setting.
If an object that was not expected to be in the enviorment, a bird for example, enters the camera's viewpoint, it can cause the model to be uncertain on what to do, since it has never seen a bird before. 
This is not because the enviorment was insufficiently specified, but because of the bird being there unexpectedly.

\section{Quantifying uncertainty} \label{ch:literature-study_section:quantifying-uncertainty}
Models who can handle / quantify the uncertainties mentioned in~\ref{ch:literature-study_section:uncertainty-in-ai} are therefore sought-after in safety-critical applications.
A way to reason about uncertainty in \glspl{dnn} is by using the Bayesian framework
The uncertainty of a given prediction of a model, also called predictive uncertainty, is depended on two things: the uncertainty of the data (data uncertainty) and the uncertainty of the model (model uncertainty)~\cite{src:survey-uncertainty-dnn}.


\section{Uncertainty-aware AI models} \label{ch:literature-study_section:uncertainty-aware-ai-models}
\subsection{Bayesian Neural Networks} \label{ch:literature-study_section:uncertainty-aware-ai-models_subsection:bayesian-neural-networks}

\subsection{Deep Ensemble Methods} \label{ch:literature-study_section:uncertainty-aware-ai-models_subsection:deep-ensemble-methods}
A deep ensemble method~\cite{src:deep-ensemble-methods} rests on the idea that a combination of (simpler) models can be trained and perform better then a single (complex) model.
Not only that, but it also outputs a great uncertainty score about its predictions.
A typical ensemble method uses multiple decision trees as their models, a deep ensemble method uses, as the name suggests, uses deep \glspl{nn} as its models.
Creating a deep ensemble method contains 3 main steps:
\begin{itemize}
    \item A proper scoring rule, which is used as a way to train the models in the ensemble.
    \item Adversarial training, which is used to make the models in the ensemble more robust.
    \item The actual ensembling, which combines the outputs of the different models to create a final output and uncertainty score.
\end{itemize}
\paragraph{The scoring rule}
A scoring rule is a way to measure the quality of a model's probabilistic predictions.
It should encourage the model to generate outputs which are accurate and are capable of telling when it is uncertain about its output.
To make this a bit more concrete, in a setting where a model needs to predict if there is a disk in frame or not, a great scoring rule should punish the model hard if it predicts a disk to be in frame with a high probability, when in fact the groud truth states there is no disk in frame.
The model should be encouraged to predict a low probability for the disk being in frame, if it is not sure if there is a disk in frame.
It should result in a model which is rightfully confident about its predictions when it is sertain about its predictions, while also being able to express its uncertainty when it is not confident in its predictions.

Fortunately, for classification tasks, the loss functions, which are commonly used to train \glspl{nn}, such as the softmax cross entropy loss, are proper scoring rules.

\paragraph{Adversarial training}
Adversarial training is a technique which is used to make \glspl{nn} more robust against small changes who can drastically change the output of the model.
Usually these changes are so small a human can not observe the difference, but the model is fooled to think it is something completely different.
For example, when classifying images of disks and cubes, when an image is classified as a disk with a high probability, it could be that with a very small change in pixel value, the network all of a sudden thinks it is a cube with high confidence.
This is something we want to avoid at all cost, since this big swing eliminates any notion of uncertainty the model could have had.

Adverserial training tries to fix these big swings by generating new training samples which are very close to some original data (for example a few pixels tweaked) and classified as some other output-class.
Using the example, this means it gets an image of a disk, tweaks a few pixels so the model thinks it is a cube, and adds this new image to the training set with the correct label (disk).
By doing this, the model learns to be more robust against these small changes, and the big swings in output are reduced.

\paragraph{Ensembling}
The final step in creating a deep ensemble method is the actual ensembling.
The paper~\cite{src:deep-ensemble-methods} suggests to create an ensemble of deep \glspl{nn} to predict an output with an uncertainty score.
Each model in the ensemble should be trained independently, however this does not mean they should be different model architectures.
In fact, when randomly initializing the \gls{nn} parameters, along with shuffeling the training data, the networks where different enough to obtain good results.
This makes that no methods like bagging or boosting are needed to create diversity in the ensemble.
Which means all the training data can be used for the training of the models in the ensemble, which is benefitial for the deep \glspl{nn} which need more data to train properly.

To get the final output and uncertainty score, the outputs of the different models are combined.
Each model in the ensemble outputs a probability distribution over the different classes.
These probability distributions are then averaged to get the final output distribution.
The label with the highest probability in this final distribution is taken as the final output with the uncertainty belonging to it.
Lets continue the example of classifying images of disks and cubes to make this more concrete.
Assuming we have an ensemble of 3 models, and they output the following probability distributions for a given input image:
\begin{itemize}
    \item Model 1: [0.7 (disk), 0.3 (cube)]
    \item Model 2: [0.4 (disk), 0.6 (cube)]
    \item Model 3: [0.9 (disk), 0.1 (cube)]
\end{itemize}
Averaging these distributions results in the final output distribution: [0.67 (disk), 0.33 (cube)].
The final output is thus 'disk' with an uncertainty score of 0.67.
\subsection{Monte Carlo dropout} \label{ch:literature-study_section:uncertainty-aware-ai-models_subsection:monte-carlo-dropout}
Dropout is a regularization technique commonly used in the training of \glspl{nn} to prevent overfitting.
The idea is to randomly drop some neurons in the network during the training phase.
This forces the model to use all the available neurons and not rely too much on a small subset of them.
When the model is fully trained and ready for inference, dropout is usually turned off, using all the neurons to make predictions.

\gls{mc} dropout~\cite{src:mc-dropout} expands on this idea by keeping dropout turned on during inference.
Every time a set of neurons is dropped, the network changes a bit, which can result in different outputs for the same input.
Each alteration of the network can be seen as a different model in an ensemble.
By letting the model make multiple forward passes with dropout turned on, we can gather a set of different outputs for the same input with their own predicted probability distributions.
These outputs can then be combined in a similar way as the deep ensemble method in~\cref{ch:literature-study_section:uncertainty-aware-ai-models_subsection:deep-ensemble-methods} to get a final output and uncertainty score.

Reusing the example of classifying images of disks and cubes, assuming we make 3 forward passes with \gls{mc} dropout, we could get the following outputs:
\begin{itemize}
    \item Forward pass 1: [0.8 (disk), 0.2 (cube)]
    \item Forward pass 2: [0.5 (disk), 0.5 (cube)]
    \item Forward pass 3: [0.6 (disk), 0.4 (cube)]
\end{itemize}
Averaging these distributions results in the final output distribution: [0.63 (disk), 0.37 (cube)].
The final output is thus 'disk' with an uncertainty score of 0.63.

\section{Datasets for uncertainty-aware models} \label{ch:literature-study_section:datasets-for-uncertainty-aware-models}

\section{Safety mechanisms for uncertain predictions} \label{ch:literature-study_section:safety-mechanisms-for-uncertain-predictions}